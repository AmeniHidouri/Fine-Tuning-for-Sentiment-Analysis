# ğŸ§  Fine-Tuning dâ€™un ModÃ¨le de Sentiment avec PEFT (LoRA)

Ce projet dÃ©montre comment fine-tuner un modÃ¨le prÃ©-entraÃ®nÃ© pour lâ€™**analyse de sentiment** Ã  lâ€™aide de techniques modernes de **PEFT (Parameter-Efficient Fine-Tuning)** et plus prÃ©cisÃ©ment **LoRA (Low-Rank Adaptation)**.  
L'objectif est dâ€™adapter `distilbert-base-uncased` sur le dataset **IMDB** pour classifier des critiques de films en *positif* ou *nÃ©gatif*.

---

## ğŸš€ Contexte Technologique

### ğŸ” Quâ€™est-ce que le Fine-Tuning ?
Le fine-tuning consiste Ã  adapter un grand modÃ¨le (comme BERT) Ã  une tÃ¢che spÃ©cifique. On continue l'entraÃ®nement du modÃ¨le sur un dataset ciblÃ© afin de lui apprendre une compÃ©tence prÃ©cise sans repartir de zÃ©ro.

### âš™ï¸ Pourquoi PEFT et LoRA ?
Fine-tuner tous les paramÃ¨tres dâ€™un modÃ¨le complet est coÃ»teux.  
PEFT permet de **geler le modÃ¨le** et de **nâ€™entraÃ®ner quâ€™une petite couche dâ€™adaptation (LoRA)**.

| Approche | Avantages |
|----------|----------|
| **LoRA (PEFT)** | ğŸ”¹ 0.1% des paramÃ¨tres entraÃ®nÃ©s<br>ğŸ”¹ EntraÃ®nement + rapide<br>ğŸ”¹ Moins de mÃ©moire GPU<br>ğŸ”¹ Pas dâ€™oubli des connaissances |

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ PrÃ©requis
- Python 3.8+
- `pip` ou `conda`

### 2ï¸âƒ£ Installation du projet

```bash
# Cloner le dÃ©pÃ´t
git clone [URL_DE_VOTRE_DEPOT_GITHUB]
cd [NOM_DU_DEPOT]
```
ğŸ“Œ Le fichier requirements.txt doit contenir :
transformers
datasets
evaluate
peft
torch
```bash
# Installer les dÃ©pendances
pip install -r requirements.txt
```
â–¶ï¸ ExÃ©cution
python fine_tune_sentiment.py

ğŸ§ª Pipeline du Script fine_tune_sentiment.py
Ã‰tape	Description
1ï¸âƒ£ Chargement des donnÃ©es	IMDB dataset via datasets
2ï¸âƒ£ ModÃ¨le de base	distilbert-base-uncased + Tokenizer
3ï¸âƒ£ Configuration LoRA	CrÃ©ation dâ€™un LoraConfig
4ï¸âƒ£ PrÃ©traitement	Tokenisation des critiques
5ï¸âƒ£ EntraÃ®nement (Trainer)	Fine-tuning PEFT (LoRA uniquement)
6ï¸âƒ£ Ã‰valuation	Accuracy sur le set de test
7ï¸âƒ£ InfÃ©rence	PrÃ©dictions sur phrases nouvelles
ğŸ“Š RÃ©sultats Attendus
Final evaluation results:
{'eval_loss': 0.35, 'eval_accuracy': 0.85, ...}


Exemples de prÃ©dictions :

Review: "This movie was fantastic!"
Prediction: POSITIVE (0.99)

Review: "I was disappointed..."
Prediction: NEGATIVE (0.99)



